{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Loading MNIST...\n",
      "└ @ Main /home/ahnaf/.julia/packages/Knet/05UDD/data/mnist.jl:33\n"
     ]
    }
   ],
   "source": [
    "using ImageMagick\n",
    "using Knet\n",
    "using AutoGrad\n",
    "using JLD\n",
    "using Statistics\n",
    "using Random\n",
    "using Images\n",
    "include(Knet.dir(\"data\", \"mnist.jl\"))\n",
    "xtrain, ytrain, xtest, ytest = mnist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Global Parameters\n",
    "BANDWIDTH = 8\n",
    "NUM_PATCHES = 1\n",
    "GLIMPSE_COUNT = 6 #Number of glimpses to be employed\n",
    "NUM_CLASSES = 10\n",
    "SCALE = 2\n",
    "HSIZE = 256\n",
    "GSIZE = 256;\n",
    "BATCHSIZE = 20\n",
    "\n",
    "# FILENAME = \"RAM_bnorm.jld2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "antinorm (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generic Functions\n",
    "# Normalize dataset and return mean and SD\n",
    "function Normalize(dataset)\n",
    "    mean_dataset = Float32(mean(dataset))\n",
    "    sd_dataset = Float32(std(dataset))\n",
    "    dataset_norm = (dataset .- mean_dataset) ./ sd_dataset\n",
    "    return mean_dataset, sd_dataset, dataset_norm\n",
    "end\n",
    "\n",
    "\n",
    "# Function for normalizing the locations\n",
    "# Batch location dimensions = 2 x Batchsize\n",
    "function antinorm(dim, loc)\n",
    "    #Conver Normalized locations range[-1,1] into X, Y coordinates\n",
    "    anloc = []\n",
    "    meanx::Int16 = floor(dim/2);\n",
    "    meany::Int16 = floor(dim/2);\n",
    "    \n",
    "    locx = Array{Int16}(ceil.((loc[1,:] .* dim) ./ 2 .+ meanx))  \n",
    "    locy = Array{Int16}(ceil.((loc[2,:] .* dim) ./ 2 .+ meany))\n",
    "    for i = 1:BATCHSIZE\n",
    "        temp = reshape([locx[i], locy[i]], (2,1))\n",
    "        push!(anloc, temp)\n",
    "    end\n",
    "    return (cat(anloc..., dims = 2))\n",
    "end\n",
    "\n",
    "# # AntiNorm test\n",
    "# test = KnetArray{Float32}(rand(2,BATCHSIZE))\n",
    "# println(size(test))\n",
    "# test = antinorm(28, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEANX, SDX, xtrain = Normalize(xtrain);\n",
    "~, ~, xtest = Normalize(xtest);\n",
    "\n",
    "dtrn = minibatch(xtrain, ytrain, BATCHSIZE, xtype=KnetArray{Float32});\n",
    "dtst = minibatch(xtest, ytest, BATCHSIZE, xtype=KnetArray{Float32});\n",
    "\n",
    "xd, yd = first(dtrn);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BatchNorm"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################################################################################################\n",
    "struct FC\n",
    "    w\n",
    "    b\n",
    "    act\n",
    "end\n",
    "\n",
    "(f::FC)(x) =f.act.(f.w * mat(x) .+ f.b)\n",
    "FC(outsize::Int, insize::Int, act = relu) = FC(param(outsize, insize, atype = KnetArray{Float32}), \n",
    "    param0(outsize, atype = KnetArray{Float32}), act);\n",
    "\n",
    "\n",
    "# # Layer Test\n",
    "# F1 = FC(100, 784)\n",
    "# params(FC1)\n",
    "# testFC = FC1(xd)\n",
    "#################################################################################################\n",
    "\n",
    "struct Linear\n",
    "    w\n",
    "    b\n",
    "end\n",
    "\n",
    "(l::Linear)(x) = l.w * mat(x) .+ l.b\n",
    "\n",
    "function Linear(outsize::Int, insize::Int)\n",
    "    w = param(outsize, insize, atype=KnetArray{Float32})\n",
    "    b = param0(outsize, atype = KnetArray{Float32})\n",
    "    return Linear(w,b)\n",
    "end\n",
    "\n",
    "##################################################################################################\n",
    "\n",
    "# # Layer Test\n",
    "# L1 = Linear(100, 784)\n",
    "# params(L1)\n",
    "# testLinear = L1(xd)\n",
    "\n",
    "##################################################################################################\n",
    "\n",
    "mutable struct BatchNorm\n",
    "    w\n",
    "    m\n",
    "end\n",
    "\n",
    "(b::BatchNorm)(x; o...) = batchnorm(x, b.m, b.w;o...)\n",
    "\n",
    "function BatchNorm(c::Int, atype = KnetArray{Float32})\n",
    "    w = Param(atype(bnparams(c)))\n",
    "    m = bnmoments()\n",
    "    return BatchNorm(w, m)\n",
    "end\n",
    "\n",
    "# Layer Test\n",
    "# btest  = BatchNorm(784)\n",
    "# res = btest(mat(xd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Network\n",
    "Takes the output of Glimpse Network(gt -> gc) and previous hidden state(h(t-1)-> hp). Outputs current Hidden State (ht -> hc).\n",
    "Runs num_glimpses times and records the hidden state at the end as output. \n",
    "Unit Defintion = Rect (Linear(h(t-1)) + Linear(gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct CoreNet\n",
    "    FCgc   # Linear operator for g current  : gt\n",
    "    FChp   # Linear operator for h previous : ht-1\n",
    "    BN_gc # BatchNormalization layer. Defined for gc\n",
    "    BN_hp # BatchNormalization layer. Defined for hp\n",
    "end\n",
    "\n",
    "function CoreNet(gsize::Int, hsize::Int)\n",
    "    return CoreNet(Linear(gsize, hsize), Linear(hsize, hsize), BatchNorm(hsize), BatchNorm(hsize))\n",
    "end\n",
    "\n",
    "function (c::CoreNet)(gc,hp)\n",
    "    return relu.(c.BN_gc(c.FCgc(gc)) + c.BN_hp(c.FChp(hp)))\n",
    "end\n",
    "\n",
    "# CoreNet Test\n",
    "# CN = CoreNet(hsize, hsize)\n",
    "# sample = KnetArray{Float32}(rand(256,10))\n",
    "# sample = reshape(sample , (256, 10))\n",
    "# CN_test = CN(sample, sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glimpse Network\n",
    "## Glimpse Network Constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct GpsNet\n",
    "    FC1\n",
    "    FC2\n",
    "    FC3\n",
    "    FC4\n",
    "    bn128_1\n",
    "    bn128_2\n",
    "    bn256_1\n",
    "    bn256_2\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glimpse Network: Initializer & Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glimpse Network default constructor: Definition of vairous cogs of the Glimpse network\n",
    "function GpsNet(bandwidth::Int, scale::Int, num_patches::Int)\n",
    "    FC1 = FC(128, bandwidth*bandwidth*num_patches, identity)\n",
    "    FC2 = FC(128, 2, identity)\n",
    "    FC3 = Linear(256, 128)  # The outsize may be 256\n",
    "    FC4 = Linear(256, 128) # The outsize may be 256\n",
    "    bn128_1 = BatchNorm(128)\n",
    "    bn128_2 = BatchNorm(128)\n",
    "    bn256_1 = BatchNorm(256)\n",
    "    bn256_2 = BatchNorm(256)\n",
    "    return GpsNet(FC1, FC2, FC3, FC4, bn128_1, bn128_2, bn256_1, bn256_2)\n",
    "end\n",
    "###########################################################################################\n",
    "\n",
    "## Glimpse Network Deployment\n",
    "\n",
    "# Deploying glimpse Network\n",
    "# Input is the batch of images\n",
    "# Take glimpses & locations. Pass them to FC layers \n",
    "function (g::GpsNet)(x_batch, loc)\n",
    "    phi   = sensor(x_batch, loc)\n",
    "    phi_1 = g.FC1(phi)\n",
    "    phi_1 = g.bn128_1(phi_1)\n",
    "    loc_1 = g.FC2(loc)\n",
    "    loc_1 = g.bn128_2(loc_1)\n",
    "    phi_2 = g.FC3(phi_1)\n",
    "    phi_2 = g.bn256_1(phi_2)\n",
    "    loc_2 = g.FC4(loc_1)\n",
    "    loc_2 = g.bn256_2(loc_2)\n",
    "    return relu.(phi_2 + loc_2) # The output here is gt to be fed to CoreNet\n",
    "end\n",
    "\n",
    "# # GpsNet Test\n",
    "# test2 = GpsNet(bandwidth, scale, num_patches)\n",
    "# params(test2.FC2)\n",
    "# res_test2 = test2(xd, loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensor Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses imresize for equalizing the return dimensions\n",
    "function sensor(x_batch, loc)\n",
    "    l,w,c,b = size(x_batch)\n",
    "#     coord = antinorm(l, loc)\n",
    "    coord = antinorm(l, loc)\n",
    "    glimpse_array = []\n",
    "    padsize::Int = 60\n",
    "    width::Int = floor(BANDWIDTH / 2)  \n",
    "    batch = Array(mat(x_batch))\n",
    "    batch = reshape(batch, (l,w,b))\n",
    "    \n",
    "    for i = 1:BATCHSIZE\n",
    "        img = batch[:,:,i]\n",
    "        pad_img = padarray(img, Fill(0, (padsize, padsize)))\n",
    "        temp = []\n",
    "        for j = 1: NUM_PATCHES\n",
    "            xlim1 = coord[1,i] - (width* (SCALE ^ (j-1)))\n",
    "            xlim2 = coord[1,i]-1 + (width * (SCALE ^(j-1)))\n",
    "            ylim1 = coord[2,i] - (width* (SCALE ^ (j-1)))\n",
    "            ylim2 = coord[2,i]-1 + (width * (SCALE ^(j-1)))\n",
    "\n",
    "            glimpse = pad_img[xlim1:xlim2, ylim1:ylim2]\n",
    "            glimpse = imresize(glimpse, (BANDWIDTH, BANDWIDTH))\n",
    "            push!(temp, glimpse)\n",
    "        end\n",
    "        temp = vcat(vcat(temp...)...)\n",
    "        push!(glimpse_array, vcat(temp))\n",
    "    end\n",
    "    return KnetArray{Float32}(mat(cat(glimpse_array..., dims=2)))\n",
    "end\n",
    "\n",
    "## Function test\n",
    "coord = KnetArray{Float32}(zeros(2, BATCHSIZE))\n",
    "res = sensor(xd, coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64×20 KnetArray{Float32,2}:\n",
       " -0.411346   2.60517   -0.424074  …   1.34511    -0.424074  -0.424074 \n",
       "  1.53602    2.78336   -0.424074      2.80882     1.16691    2.08332  \n",
       "  2.79609    2.78336   -0.424074      2.41425     2.66881    2.78336  \n",
       "  0.721438   1.85422   -0.424074     -0.105876    0.212322   1.89241  \n",
       " -0.424074  -0.271339  -0.424074     -0.424074   -0.424074  -0.424074 \n",
       " -0.424074   0.530519  -0.424074  …  -0.424074   -0.424074  -0.424074 \n",
       " -0.424074   1.116     -0.424074      0.0341309  -0.424074  -0.424074 \n",
       " -0.424074  -0.156788  -0.424074      1.73967    -0.424074   0.0468588\n",
       " -0.424074   2.79609   -0.424074      0.275961   -0.424074  -0.424074 \n",
       "  1.34511    2.54153   -0.424074      2.27424     2.05787    1.96877  \n",
       "  2.79609    2.22333   -0.424074  …   2.79609     1.91786    2.78336  \n",
       "  1.99423    0.645071  -0.424074      1.62512    -0.424074   0.886901 \n",
       " -0.398618  -0.424074  -0.424074     -0.284067   -0.424074  -0.424074 \n",
       "  ⋮                               ⋱                                   \n",
       " -0.424074  -0.424074  -0.424074      2.79609    -0.182244   2.79609  \n",
       " -0.220427  -0.424074  -0.424074      1.84149    -0.424074   2.78336  \n",
       "  0.759622  -0.424074  -0.424074     -0.424074   -0.424074   0.0595867\n",
       "  2.78336   -0.424074   0.874173  …  -0.424074   -0.424074  -0.424074 \n",
       " -0.424074  -0.424074  -0.424074      2.80882     2.80882   -0.424074 \n",
       " -0.424074  -0.424074  -0.424074      2.79609    -0.258611  -0.424074 \n",
       " -0.424074  -0.424074  -0.424074      2.15969    -0.424074  -0.424074 \n",
       " -0.424074  -0.424074  -0.424074      1.56148    -0.424074   0.70871  \n",
       " -0.424074  -0.424074  -0.424074  …   2.79609    -0.424074   2.82154  \n",
       " -0.424074  -0.424074  -0.424074      2.12151    -0.424074   2.79609  \n",
       " -0.424074  -0.424074  -0.424074     -0.424074   -0.424074   0.0595867\n",
       "  2.74518   -0.334978   1.72694      -0.424074   -0.424074  -0.424074 "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uses the pooling for equalizing dimensions\n",
    "function sensor(x_batch, loc)\n",
    "    patch_size =  BANDWIDTH\n",
    "    phi = []\n",
    "    for i = 1:NUM_PATCHES\n",
    "        push!(phi, foveat(x_batch, loc, patch_size))\n",
    "        patch_size = SCALE * patch_size\n",
    "    end\n",
    "    \n",
    "    for j = 1:length(phi)\n",
    "        k = div(size(phi[j])[1], BANDWIDTH)\n",
    "        phi[j] = pool(phi[j]; window = k, mode=1)\n",
    "    end\n",
    "    \n",
    "    phi2d = map(mat, phi)\n",
    "    return mat(vcat(phi2d...))\n",
    "end\n",
    "\n",
    "function foveat(x_batch, loc, patch_size)\n",
    "    l,w,c,b = size(x_batch)\n",
    "    coord = antinorm(l, loc)\n",
    "    patches = [] \n",
    "    padsize::Int = 60\n",
    "    width::Int = floor(patch_size / 2)\n",
    "\n",
    "    xlim1 = coord[1,:] .- width .+1\n",
    "    xlim2 = coord[1,:] .+ width\n",
    "    ylim1 = coord[2,:] .- width .+1\n",
    "    ylim2 = coord[2,:] .+ width \n",
    "    batch = Array(x_batch)\n",
    "\n",
    "    for i=1:b\n",
    "        img = batch[:,:,1,i]\n",
    "        pad_img = padarray(img, Fill(0, (padsize, padsize)))\n",
    "        push!(patches, pad_img[xlim1[i]:xlim2[i], ylim1[i]:ylim2[i]])\n",
    "    end\n",
    "    \n",
    "    return KnetArray{Float32}(cat(patches...; dims=4))\n",
    "end          \n",
    "\n",
    "# ## Function test\n",
    "coord = KnetArray{Float32}(zeros(2, BATCHSIZE))\n",
    "res = sensor(xd, coord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct LocNet\n",
    "    sdx\n",
    "    Linl\n",
    "end\n",
    "\n",
    "# Function should take input size as 256 and output size as 2 (size of loc coordinates)\n",
    "function LocNet(outsize::Int, insize::Int, sdx::Float32)\n",
    "    Linloc = Linear(outsize, insize)\n",
    "    return LocNet(sdx, Linloc)\n",
    "end\n",
    "\n",
    "function (l::LocNet)(ht)\n",
    "    mean_loc = tanh.(l.Linl(ht))\n",
    "    \n",
    "    prtb = KnetArray{Float32}(gaussian((2,BATCHSIZE); mean = 0, std = 0.12))\n",
    "    loc_new = mean_loc + prtb \n",
    "    return mean_loc, value(tanh.(loc_new))\n",
    "end\n",
    "\n",
    "# # TLocation Network Test\n",
    "# Loc1 = LocNet(2, 256, SDX)\n",
    "# sample = KnetArray{Float32}(rand(256));\n",
    "# sample = reshape(sample, (256, 1));\n",
    "# res, lt = Loc1(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAM Model\n",
    "### Staggered RAM\n",
    "The Reference RAM deployment has been staggered.\n",
    "Deploy one RAM function once to generate the hidden state, next location and action.\n",
    "Deploy this one-time active RAM recursively equal to number of glimpses. Record all intermediate states\n",
    "but base decision on output of the final one of the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAM Initializer\n",
    "# ram = RAM(SDX, GLIMPSE_COUNT, BANDWIDTH, NUM_PATCHES, NUM_CLASSES, SCALE, GSIZE, HSIZE);\n",
    "mutable struct RAM\n",
    "    sdx\n",
    "    num_glimpses\n",
    "    gpsnet::GpsNet\n",
    "    corenet::CoreNet\n",
    "    locnet::LocNet\n",
    "    output_layer\n",
    "    locations\n",
    "    basenet\n",
    "    hsize\n",
    "end\n",
    "\n",
    "function RAM(sdx, glimpse_count, bandwidth, num_patches, num_classes, scale, gsize, hsize)\n",
    "#     sdx = SDX\n",
    "#     num_glimpses = glimpse_count\n",
    "    gpsnet = GpsNet(bandwidth, scale, num_patches)\n",
    "    corenet = CoreNet(gsize, hsize)\n",
    "    locnet = LocNet(2, hsize, sdx)\n",
    "    output_layer = Linear(num_classes, hsize) # Action Network\n",
    "    locations = []\n",
    "    basenet = FC(1, hsize)\n",
    "    return RAM(sdx, glimpse_count, gpsnet, corenet, locnet, output_layer, locations, basenet, hsize)\n",
    "end\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "# Outer RAM instance. Calls inner instance glimpse_count times\n",
    "# Returns class_probs, policy result, hidden state and baseline results\n",
    "\n",
    "function (r::RAM)(x_batch)\n",
    "    # Init States\n",
    "    hc = KnetArray{Float32}(reshape(zeros(r.hsize, BATCHSIZE), (r.hsize, BATCHSIZE)))\n",
    "    loc = KnetArray{Float32}(reshape(zeros(2,BATCHSIZE), (2, BATCHSIZE)))\n",
    "    \n",
    "    xs, log_ps, baseresults = [], [], [] \n",
    "    for i = 1:r.num_glimpses\n",
    "        hc, loc, policy, baseres = r(x_batch, loc, hc)\n",
    "        push!(log_ps, policy)       \n",
    "        push!(baseresults, baseres)      \n",
    "    end\n",
    "    log_ps, baseresults = vcat(log_ps...), vcat(baseresults...)    \n",
    "    scores = logsoftmax(r.output_layer(hc))  # Size of NUM_CLASSES x BATCHSIZE. mat() applied at layer definition\n",
    "    return scores, log_ps, hc, baseresults\n",
    "end\n",
    "\n",
    "###########################################################################################\n",
    "\n",
    "# Innermost RAM instance\n",
    "# Return for a single glimpse, hidden state, next_location, policy and baseline results.\n",
    "function (r::RAM)(x_batch, loc, hp)\n",
    "    gc = r.gpsnet(x_batch, loc)  # g-current: gt\n",
    "    hc = r.corenet(gc,hp)       # h-current ; ht-- Uses g_current and h_previous\n",
    "    mean, lnext = r.locnet(hc) # mean and l-next:lt to be used in the next time step\n",
    "    sdx = r.sdx\n",
    "    sdx2 = sdx.^2\n",
    "    policy = -(abs.(lnext - mean).^2) / 2*(sdx2) .- log(sdx) .- log(sqrt(2 * 3.1415))\n",
    "    policy = sum(policy, dims =1)\n",
    "    base_res = r.basenet(hc)\n",
    "    return hc, lnext, policy, base_res\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Caluclator\n",
    "function (ram::RAM)(x_batch, y)\n",
    "    scores, log_ps, hc, baseresults = ram(x_batch) \n",
    "    ypred = vec(map(i->i[1], argmax(Array(value(scores)), dims=1)))\n",
    "#     ypred = vec(map(i->i[1], argmax(scores, dims=1)))\n",
    "    r = ypred .== y; r = reshape(r, 1, :) # Return Boolean values\n",
    "#     r = Array{Float32}(ypred .== y)\n",
    "#     r = reshape(r, (1, BATCHSIZE))\n",
    "    \n",
    "    # Cumulation over one-batch\n",
    "    R = zeros(Float32, size(baseresults)...); R[end,:] = r\n",
    "    R = KnetArray{Float32}(R)\n",
    "#     R̂ = R .- value(baseresults)\n",
    "    R_err = R .- value(baseresults)\n",
    "    loss_action = nll(scores, y)\n",
    "    loss_baseline = sum(abs2, baseresults .- R) / length(baseresults)\n",
    "    loss_reinforce = mean(sum(-log_ps .* R_err, dims=1))\n",
    "    return loss_action, loss_baseline, loss_reinforce, sum(R), length(R)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "validate (generic function with 1 method)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function validate(ram::RAM, data)\n",
    "    losses = zeros(3)\n",
    "    ncorrect = ninstances = 0\n",
    "    for (x,y) in data\n",
    "        ret = ram(x,y)\n",
    "        for i = 1:3; losses[i] += ret[i]; end\n",
    "        ncorrect += ret[4]\n",
    "        ninstances += ret[5]\n",
    "    end\n",
    "    losses = losses / length(data)\n",
    "    losses = [sum(losses), losses...]\n",
    "    return losses, ncorrect / ninstances\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAM Initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "function initopt!(model, optimizer=\"Adam(lr=0.003, gclip=0.0)\")\n",
    "    for par in params(model)\n",
    "        par.opt = eval(Meta.parse(optimizer))\n",
    "    end\n",
    "end\n",
    "\n",
    "ram = RAM(SDX, GLIMPSE_COUNT, BANDWIDTH, NUM_PATCHES, NUM_CLASSES, SCALE, GSIZE, HSIZE);\n",
    "record = params(ram);\n",
    "initopt!(ram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ram = Knet.load(\"RAM_bnorm.jld2\", \"RAM\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for #epoch : 0.73197216\n",
      "epoch=1 trnloss=[0.61151, 0.868648, 0.122842, -0.37998], trnacc=0.12284166, tstloss=[0.579599, 0.840164, 0.124483, -0.385049], tstacc=0.12448333\n",
      "Loss for #epoch : 0.40665886\n",
      "epoch=2 trnloss=[0.440806, 0.713211, 0.130122, -0.402527], trnacc=0.13012223, tstloss=[0.433509, 0.706696, 0.1305, -0.403687], tstacc=0.1305\n",
      "Loss for #epoch : 0.28901953\n",
      "epoch=3 trnloss=[0.303507, 0.589309, 0.136525, -0.422327], trnacc=0.136525, tstloss=[0.308736, 0.594618, 0.136567, -0.422449], tstacc=0.13656667\n",
      "Loss for #epoch : 0.21611997\n",
      "epoch=4 trnloss=[0.325748, 0.610021, 0.1358, -0.420073], trnacc=0.1358, tstloss=[0.353135, 0.635637, 0.13495, -0.417452], tstacc=0.13495\n",
      "Loss for #epoch : 0.16087914\n",
      "epoch=5 trnloss=[0.18978, 0.483857, 0.140478, -0.434555], trnacc=0.14047778, tstloss=[0.220594, 0.512466, 0.139417, -0.431288], tstacc=0.13941666\n",
      "Loss for #epoch : 0.13115229\n",
      "epoch=6 trnloss=[0.0956406, 0.399614, 0.145217, -0.44919], trnacc=0.14521667, tstloss=[0.14063, 0.441116, 0.14355, -0.444037], tstacc=0.14355\n",
      "Loss for #epoch : 0.09737237\n",
      "epoch=7 trnloss=[0.0690342, 0.374267, 0.145828, -0.45106], trnacc=0.14582779, tstloss=[0.116606, 0.419491, 0.144717, -0.447602], tstacc=0.14471667\n",
      "Loss for #epoch : 0.07755186\n",
      "epoch=8 trnloss=[0.0400891, 0.349013, 0.147586, -0.45651], trnacc=0.1475861, tstloss=[0.112036, 0.414548, 0.144517, -0.447029], tstacc=0.14451666\n",
      "Loss for #epoch : 0.045067545\n",
      "epoch=9 trnloss=[0.0184061, 0.329844, 0.148789, -0.460227], trnacc=0.14878888, tstloss=[0.0881133, 0.394076, 0.146167, -0.452129], tstacc=0.14616667\n",
      "Loss for #epoch : 0.03951508\n",
      "epoch=10 trnloss=[0.0553973, 0.363687, 0.147286, -0.455576], trnacc=0.14728612, tstloss=[0.138483, 0.440859, 0.144467, -0.446843], tstacc=0.14446667\n",
      "Loss for #epoch : 0.02339672\n",
      "epoch=11 trnloss=[0.00358102, 0.316394, 0.149436, -0.462249], trnacc=0.14943612, tstloss=[0.0955832, 0.40214, 0.14645, -0.453007], tstacc=0.14645\n",
      "Loss for #epoch : 0.013898941\n",
      "epoch=12 trnloss=[-0.0133466, 0.301467, 0.150397, -0.46521], trnacc=0.15039723, tstloss=[0.0865467, 0.39411, 0.146933, -0.454497], tstacc=0.14693333\n",
      "Loss for #epoch : 0.0034883695\n",
      "epoch=13 trnloss=[0.0174915, 0.328821, 0.148761, -0.46009], trnacc=0.14876111, tstloss=[0.109443, 0.415569, 0.146267, -0.452393], tstacc=0.14626667\n",
      "Loss for #epoch : -0.0056760004\n",
      "epoch=14 trnloss=[-0.00161625, 0.311584, 0.149625, -0.462825], trnacc=0.149625, tstloss=[0.105735, 0.411667, 0.146167, -0.452098], tstacc=0.14616667\n",
      "Loss for #epoch : -0.014376543\n",
      "epoch=15 trnloss=[0.0178393, 0.32925, 0.148775, -0.460185], trnacc=0.148775, tstloss=[0.14286, 0.445761, 0.144717, -0.447618], tstacc=0.14471667\n",
      "Loss for #epoch : -0.034816522\n",
      "epoch=16 trnloss=[-0.052297, 0.266225, 0.152175, -0.470697], trnacc=0.152175, tstloss=[0.0599177, 0.369788, 0.14805, -0.457921], tstacc=0.14805\n",
      "Loss for #epoch : -0.039269306\n",
      "epoch=17 trnloss=[0.00794286, 0.322071, 0.150072, -0.464201], trnacc=0.15007222, tstloss=[0.157131, 0.461239, 0.145283, -0.449392], tstacc=0.14528333\n",
      "Loss for #epoch : -0.046745803\n",
      "epoch=18 trnloss=[-0.0641594, 0.255337, 0.15265, -0.472147], trnacc=0.15265, tstloss=[0.068363, 0.378885, 0.148367, -0.458889], tstacc=0.14836666\n",
      "Loss for #epoch : -0.056019112\n",
      "epoch=19 trnloss=[-0.0716062, 0.249191, 0.153264, -0.474062], trnacc=0.15326388, tstloss=[0.073712, 0.383424, 0.147967, -0.457679], tstacc=0.14796667\n",
      "Loss for #epoch : -0.058391657\n",
      "epoch=20 trnloss=[-0.034949, 0.281788, 0.151317, -0.468054], trnacc=0.15131667, tstloss=[0.108481, 0.415197, 0.146533, -0.453249], tstacc=0.14653334\n",
      "Loss for #epoch : -0.06831003\n",
      "epoch=21 trnloss=[-0.0761575, 0.24443, 0.153164, -0.473751], trnacc=0.1531639, tstloss=[0.0748467, 0.385349, 0.14835, -0.458852], tstacc=0.14835\n",
      "Loss for #epoch : -0.08527895\n",
      "epoch=22 trnloss=[-0.104772, 0.218643, 0.154511, -0.477926], trnacc=0.15451111, tstloss=[0.06577, 0.378205, 0.149267, -0.461701], tstacc=0.14926666\n",
      "Loss for #epoch : -0.0929092\n",
      "epoch=23 trnloss=[-0.0709143, 0.249026, 0.152856, -0.472796], trnacc=0.15285556, tstloss=[0.11491, 0.423178, 0.147283, -0.455552], tstacc=0.14728333\n",
      "Loss for #epoch : -0.09563788\n",
      "epoch=24 trnloss=[-0.077783, 0.243382, 0.153439, -0.474604], trnacc=0.1534389, tstloss=[0.109287, 0.418477, 0.147717, -0.456907], tstacc=0.14771667\n",
      "Loss for #epoch : -0.097684875\n",
      "epoch=25 trnloss=[-0.0501921, 0.268543, 0.152269, -0.471005], trnacc=0.15226944, tstloss=[0.179406, 0.483855, 0.14545, -0.449899], tstacc=0.14545\n",
      "Loss for #epoch : -0.10606845\n",
      "epoch=26 trnloss=[-0.0865067, 0.235251, 0.153722, -0.47548], trnacc=0.15372223, tstloss=[0.114269, 0.421813, 0.146933, -0.454477], tstacc=0.14693333\n",
      "Loss for #epoch : -0.10790735\n",
      "epoch=27 trnloss=[-0.0956843, 0.227205, 0.154258, -0.477148], trnacc=0.15425834, tstloss=[0.114226, 0.424422, 0.1482, -0.458396], tstacc=0.1482\n",
      "Loss for #epoch : -0.10664438\n",
      "epoch=28 trnloss=[-0.112827, 0.211611, 0.155003, -0.47944], trnacc=0.15500277, tstloss=[0.097688, 0.409241, 0.14885, -0.460403], tstacc=0.14885\n",
      "Loss for #epoch : -0.12477011\n",
      "epoch=29 trnloss=[-0.105957, 0.217561, 0.154558, -0.478076], trnacc=0.15455833, tstloss=[0.088798, 0.400011, 0.148683, -0.459896], tstacc=0.14868334\n",
      "Loss for #epoch : -0.12158404\n",
      "epoch=30 trnloss=[-0.109402, 0.214419, 0.154706, -0.478526], trnacc=0.15470555, tstloss=[0.128681, 0.438358, 0.14795, -0.457627], tstacc=0.14795\n",
      "Loss for #epoch : -0.12084719\n",
      "epoch=31 trnloss=[-0.127849, 0.198137, 0.155742, -0.481727], trnacc=0.15574166, tstloss=[0.10078, 0.412911, 0.149117, -0.461247], tstacc=0.14911667\n",
      "Loss for #epoch : -0.1343278\n",
      "epoch=32 trnloss=[-0.163459, 0.166028, 0.157425, -0.486913], trnacc=0.157425, tstloss=[0.0781965, 0.390521, 0.149217, -0.461541], tstacc=0.14921667\n",
      "Loss for #epoch : -0.13856797\n",
      "epoch=33 trnloss=[-0.155754, 0.172985, 0.157056, -0.485795], trnacc=0.15705556, tstloss=[0.104965, 0.417711, 0.149417, -0.462162], tstacc=0.14941667\n",
      "Loss for #epoch : -0.14459029\n",
      "epoch=34 trnloss=[-0.138476, 0.188499, 0.156208, -0.483184], trnacc=0.15620834, tstloss=[0.131361, 0.4414, 0.148117, -0.458155], tstacc=0.14811666\n",
      "Loss for #epoch : -0.14819187\n",
      "epoch=35 trnloss=[-0.154083, 0.173829, 0.156661, -0.484573], trnacc=0.15666111, tstloss=[0.127489, 0.437763, 0.148233, -0.458507], tstacc=0.14823334\n",
      "Loss for #epoch : -0.14617315\n",
      "epoch=36 trnloss=[-0.149018, 0.178791, 0.156614, -0.484424], trnacc=0.15661389, tstloss=[0.145356, 0.455779, 0.1483, -0.458723], tstacc=0.1483\n",
      "Loss for #epoch : -0.15035951\n",
      "epoch=37 trnloss=[-0.154323, 0.174305, 0.157006, -0.485634], trnacc=0.15700555, tstloss=[0.138267, 0.448968, 0.148433, -0.459134], tstacc=0.14843333\n",
      "Loss for #epoch : -0.15633877\n",
      "epoch=38 trnloss=[-0.153245, 0.175433, 0.157028, -0.485706], trnacc=0.15702778, tstloss=[0.139978, 0.45267, 0.149383, -0.462075], tstacc=0.14938334\n",
      "Loss for #epoch : -0.16296834\n",
      "epoch=39 trnloss=[-0.145468, 0.182407, 0.156642, -0.484516], trnacc=0.15664166, tstloss=[0.145426, 0.455673, 0.148233, -0.45848], tstacc=0.14823334\n",
      "Loss for #epoch : -0.16734688\n",
      "epoch=40 trnloss=[-0.182018, 0.149233, 0.158261, -0.489513], trnacc=0.1582611, tstloss=[0.136696, 0.44971, 0.14955, -0.462564], tstacc=0.14955\n",
      "Loss for #epoch : -0.16318028\n",
      "epoch=41 trnloss=[-0.14854, 0.179098, 0.156556, -0.484193], trnacc=0.15655555, tstloss=[0.209462, 0.517506, 0.147183, -0.455227], tstacc=0.14718333\n",
      "Loss for #epoch : -0.16415305\n",
      "epoch=42 trnloss=[-0.139437, 0.187329, 0.156117, -0.482883], trnacc=0.15611666, tstloss=[0.210456, 0.517034, 0.146467, -0.453045], tstacc=0.14646667\n",
      "Loss for #epoch : -0.1665832\n",
      "epoch=43 trnloss=[-0.141926, 0.185023, 0.1562, -0.483149], trnacc=0.1562, tstloss=[0.213856, 0.520892, 0.146683, -0.45372], tstacc=0.14668334\n",
      "Loss for #epoch : -0.17509264\n",
      "epoch=44 trnloss=[-0.176625, 0.154285, 0.158097, -0.489008], trnacc=0.15809722, tstloss=[0.188417, 0.498341, 0.148083, -0.458007], tstacc=0.14808333\n",
      "Loss for #epoch : -0.1706036\n",
      "epoch=45 trnloss=[-0.159399, 0.170195, 0.157478, -0.487072], trnacc=0.15747778, tstloss=[0.21319, 0.521539, 0.147317, -0.455666], tstacc=0.14731666\n",
      "Loss for #epoch : -0.17472512\n",
      "epoch=46 trnloss=[-0.151359, 0.177362, 0.15705, -0.485771], trnacc=0.15705, tstloss=[0.22953, 0.538263, 0.1475, -0.456233], tstacc=0.1475\n",
      "Loss for #epoch : -0.17333356\n",
      "epoch=47 trnloss=[-0.15375, 0.174472, 0.156836, -0.485059], trnacc=0.1568361, tstloss=[0.230765, 0.537255, 0.14645, -0.45294], tstacc=0.14645\n",
      "Loss for #epoch : -0.17627901\n",
      "epoch=48 trnloss=[-0.179062, 0.151803, 0.158075, -0.48894], trnacc=0.158075, tstloss=[0.219302, 0.528623, 0.147783, -0.457104], tstacc=0.14778334\n",
      "Loss for #epoch : -0.18790515\n",
      "epoch=49 trnloss=[-0.174953, 0.155246, 0.15775, -0.48795], trnacc=0.15775, tstloss=[0.238062, 0.545247, 0.146767, -0.453952], tstacc=0.14676666\n",
      "Loss for #epoch : -0.1846975\n",
      "epoch=50 trnloss=[-0.164407, 0.165273, 0.157514, -0.487194], trnacc=0.15751389, tstloss=[0.248145, 0.556765, 0.14745, -0.45607], tstacc=0.14745\n",
      "Loss for #epoch : -0.18784231\n",
      "epoch=51 trnloss=[-0.203785, 0.129722, 0.159339, -0.492846], trnacc=0.15933889, tstloss=[0.213437, 0.525398, 0.14905, -0.461011], tstacc=0.14905\n",
      "Loss for #epoch : -0.19546238\n",
      "epoch=52 trnloss=[-0.20263, 0.130696, 0.159258, -0.492584], trnacc=0.15925834, tstloss=[0.241893, 0.551255, 0.147817, -0.457179], tstacc=0.14781667\n",
      "Loss for #epoch : -0.19171175\n",
      "epoch=53 trnloss=[-0.170625, 0.16028, 0.158081, -0.488985], trnacc=0.15808056, tstloss=[0.301611, 0.608658, 0.146683, -0.45373], tstacc=0.14668334\n",
      "Loss for #epoch : -0.19844356\n",
      "epoch=54 trnloss=[-0.192565, 0.140252, 0.159006, -0.491823], trnacc=0.15900555, tstloss=[0.257025, 0.567621, 0.148383, -0.458979], tstacc=0.14838333\n",
      "Loss for #epoch : -0.20014475\n",
      "epoch=55 trnloss=[-0.183069, 0.148227, 0.158283, -0.48958], trnacc=0.15828334, tstloss=[0.270149, 0.576908, 0.146567, -0.453326], tstacc=0.14656667\n",
      "Loss for #epoch : -0.20616437\n",
      "epoch=56 trnloss=[-0.182403, 0.148656, 0.158169, -0.489229], trnacc=0.15816945, tstloss=[0.259563, 0.567224, 0.146983, -0.454644], tstacc=0.14698334\n",
      "Loss for #epoch : -0.20258838\n",
      "epoch=57 trnloss=[-0.224301, 0.110673, 0.16005, -0.495024], trnacc=0.16005, tstloss=[0.21398, 0.526283, 0.149217, -0.46152], tstacc=0.14921667\n",
      "Loss for #epoch : -0.20411229\n",
      "epoch=58 trnloss=[-0.175552, 0.154832, 0.157842, -0.488225], trnacc=0.15784167, tstloss=[0.332035, 0.638972, 0.14665, -0.453587], tstacc=0.14665\n",
      "Loss for #epoch : -0.20613834\n",
      "epoch=59 trnloss=[-0.211133, 0.123395, 0.159825, -0.494353], trnacc=0.159825, tstloss=[0.225645, 0.536467, 0.1485, -0.459322], tstacc=0.1485\n",
      "Loss for #epoch : -0.20131893\n",
      "epoch=60 trnloss=[-0.161328, 0.1678, 0.157283, -0.486411], trnacc=0.15728334, tstloss=[0.365083, 0.667765, 0.14465, -0.447332], tstacc=0.14465\n",
      "Loss for #epoch : -0.21152821\n",
      "epoch=61 trnloss=[-0.214315, 0.120169, 0.159817, -0.494301], trnacc=0.15981667, tstloss=[0.275713, 0.584939, 0.14775, -0.456976], tstacc=0.14775\n",
      "Loss for #epoch : -0.21486987\n",
      "epoch=62 trnloss=[-0.205278, 0.129123, 0.159783, -0.494184], trnacc=0.15978333, tstloss=[0.320237, 0.628548, 0.147317, -0.455628], tstacc=0.14731666\n",
      "Loss for #epoch : -0.20677824\n",
      "epoch=63 trnloss=[-0.196097, 0.136892, 0.159114, -0.492103], trnacc=0.15911388, tstloss=[0.341806, 0.648046, 0.146333, -0.452573], tstacc=0.14633334\n",
      "Loss for #epoch : -0.21121657\n",
      "epoch=64 trnloss=[-0.215991, 0.118981, 0.160042, -0.495014], trnacc=0.16004166, tstloss=[0.272771, 0.584268, 0.148817, -0.460314], tstacc=0.14881666\n",
      "Loss for #epoch : -0.21198785\n",
      "epoch=65 trnloss=[-0.201194, 0.132318, 0.159353, -0.492865], trnacc=0.15935278, tstloss=[0.313894, 0.622901, 0.14765, -0.456657], tstacc=0.14765\n",
      "Loss for #epoch : -0.21246721\n",
      "epoch=66 trnloss=[-0.231121, 0.104724, 0.160464, -0.496308], trnacc=0.16046388, tstloss=[0.274314, 0.583543, 0.14775, -0.456979], tstacc=0.14775\n",
      "Loss for #epoch : -0.2252963\n",
      "epoch=67 trnloss=[-0.21988, 0.11531, 0.160147, -0.495338], trnacc=0.16014722, tstloss=[0.337563, 0.646064, 0.147383, -0.455885], tstacc=0.14738333\n",
      "Loss for #epoch : -0.2142605\n",
      "epoch=68 trnloss=[-0.191326, 0.140915, 0.158731, -0.490972], trnacc=0.15873055, tstloss=[0.412468, 0.717878, 0.145917, -0.451327], tstacc=0.14591667\n",
      "Loss for #epoch : -0.22634065\n",
      "epoch=69 trnloss=[-0.217976, 0.116835, 0.159967, -0.494778], trnacc=0.15996666, tstloss=[0.327953, 0.634844, 0.146633, -0.453524], tstacc=0.14663333\n",
      "Loss for #epoch : -0.22473162\n",
      "epoch=70 trnloss=[-0.233353, 0.103337, 0.160894, -0.497585], trnacc=0.16089444, tstloss=[0.316143, 0.627049, 0.148583, -0.459489], tstacc=0.14858334\n",
      "Loss for #epoch : -0.22359712\n",
      "epoch=71 trnloss=[-0.181492, 0.149958, 0.158369, -0.48982], trnacc=0.15836945, tstloss=[0.433235, 0.736265, 0.1448, -0.447829], tstacc=0.1448\n",
      "Loss for #epoch : -0.23623264\n",
      "epoch=72 trnloss=[-0.221682, 0.114192, 0.160472, -0.496346], trnacc=0.16047223, tstloss=[0.348836, 0.658375, 0.147883, -0.457422], tstacc=0.14788334\n",
      "Loss for #epoch : -0.22796902\n",
      "epoch=73 trnloss=[-0.238594, 0.0980284, 0.160842, -0.497464], trnacc=0.16084167, tstloss=[0.316174, 0.625835, 0.147967, -0.457628], tstacc=0.14796667\n",
      "Loss for #epoch : -0.22495118\n",
      "epoch=74 trnloss=[-0.227174, 0.109548, 0.160892, -0.497614], trnacc=0.16089167, tstloss=[0.366688, 0.674303, 0.146983, -0.454599], tstacc=0.14698334\n",
      "Loss for #epoch : -0.23326705\n",
      "epoch=75 trnloss=[-0.246169, 0.0920007, 0.161569, -0.49974], trnacc=0.16156945, tstloss=[0.322432, 0.632953, 0.148367, -0.458888], tstacc=0.14836666\n",
      "Loss for #epoch : -0.23238528\n",
      "epoch=76 trnloss=[-0.228656, 0.107965, 0.160831, -0.497452], trnacc=0.16083056, tstloss=[0.424392, 0.73185, 0.1469, -0.454358], tstacc=0.1469\n",
      "Loss for #epoch : -0.23130803\n",
      "epoch=77 trnloss=[-0.219693, 0.116052, 0.160408, -0.496153], trnacc=0.16040833, tstloss=[0.413623, 0.721513, 0.1471, -0.45499], tstacc=0.1471\n",
      "Loss for #epoch : -0.23547404\n",
      "epoch=78 trnloss=[-0.217871, 0.11755, 0.160275, -0.495695], trnacc=0.160275, tstloss=[0.444798, 0.750639, 0.146133, -0.451975], tstacc=0.14613333\n",
      "Loss for #epoch : -0.23806052\n",
      "epoch=79 trnloss=[-0.236613, 0.100335, 0.161006, -0.497953], trnacc=0.16100556, tstloss=[0.415618, 0.724026, 0.147367, -0.455775], tstacc=0.14736667\n",
      "Loss for #epoch : -0.2357086\n",
      "epoch=80 trnloss=[-0.235027, 0.101728, 0.160892, -0.497646], trnacc=0.16089167, tstloss=[0.406258, 0.714646, 0.147333, -0.455721], tstacc=0.14733334\n",
      "Loss for #epoch : -0.2386906\n",
      "epoch=81 trnloss=[-0.211144, 0.123356, 0.159881, -0.494381], trnacc=0.15988055, tstloss=[0.421843, 0.726455, 0.1456, -0.450211], tstacc=0.1456\n",
      "Loss for #epoch : -0.2389327\n",
      "epoch=82 trnloss=[-0.195372, 0.137109, 0.158878, -0.491359], trnacc=0.15887778, tstloss=[0.501602, 0.803456, 0.14425, -0.446104], tstacc=0.14425\n",
      "Loss for #epoch : -0.23673442\n",
      "epoch=83 trnloss=[-0.239674, 0.0977531, 0.161225, -0.498653], trnacc=0.161225, tstloss=[0.417765, 0.726231, 0.147383, -0.455849], tstacc=0.14738333\n",
      "Loss for #epoch : -0.2348836\n",
      "epoch=84 trnloss=[-0.254248, 0.0845828, 0.161883, -0.500714], trnacc=0.16188334, tstloss=[0.371121, 0.680086, 0.147617, -0.456582], tstacc=0.14761667\n",
      "Loss for #epoch : -0.2448329\n",
      "epoch=85 trnloss=[-0.24768, 0.0903862, 0.161533, -0.4996], trnacc=0.16153333, tstloss=[0.435473, 0.74369, 0.147283, -0.455501], tstacc=0.14728333\n",
      "Loss for #epoch : -0.24371965\n",
      "epoch=86 trnloss=[-0.240879, 0.0967629, 0.161317, -0.498959], trnacc=0.16131666, tstloss=[0.459485, 0.767323, 0.147083, -0.454921], tstacc=0.14708333\n",
      "Loss for #epoch : -0.24319443\n",
      "epoch=87 trnloss=[-0.230248, 0.106659, 0.160958, -0.497865], trnacc=0.16095833, tstloss=[0.477117, 0.781455, 0.1454, -0.449738], tstacc=0.1454\n",
      "Loss for #epoch : -0.24457482\n",
      "epoch=88 trnloss=[-0.220942, 0.114057, 0.160058, -0.495057], trnacc=0.16005833, tstloss=[0.493935, 0.797042, 0.144817, -0.447924], tstacc=0.14481667\n",
      "Loss for #epoch : -0.24734446\n",
      "epoch=89 trnloss=[-0.223277, 0.112806, 0.160625, -0.496708], trnacc=0.160625, tstloss=[0.482539, 0.787559, 0.145783, -0.450803], tstacc=0.14578333\n",
      "Loss for #epoch : -0.24330555\n",
      "epoch=90 trnloss=[-0.239762, 0.0978785, 0.161344, -0.498985], trnacc=0.16134444, tstloss=[0.438017, 0.746096, 0.147217, -0.455296], tstacc=0.14721666\n",
      "Loss for #epoch : -0.25035727\n",
      "epoch=91 trnloss=[-0.25672, 0.0823155, 0.162003, -0.501038], trnacc=0.16200277, tstloss=[0.421262, 0.731381, 0.148183, -0.458303], tstacc=0.14818333\n",
      "Loss for #epoch : -0.24822453\n",
      "epoch=92 trnloss=[-0.232598, 0.104699, 0.161203, -0.498499], trnacc=0.16120277, tstloss=[0.484748, 0.790885, 0.146317, -0.452454], tstacc=0.14631666\n",
      "Loss for #epoch : -0.24108273\n",
      "epoch=93 trnloss=[-0.253191, 0.0859286, 0.162033, -0.501153], trnacc=0.16203333, tstloss=[0.410521, 0.720632, 0.148167, -0.458277], tstacc=0.14816667\n",
      "Loss for #epoch : -0.25123262\n",
      "epoch=94 trnloss=[-0.202924, 0.13111, 0.159603, -0.493636], trnacc=0.15960278, tstloss=[0.574126, 0.87648, 0.144467, -0.446822], tstacc=0.14446667\n",
      "Loss for #epoch : -0.25476995\n",
      "epoch=95 trnloss=[-0.247985, 0.0904313, 0.161708, -0.500125], trnacc=0.16170834, tstloss=[0.475013, 0.782231, 0.1468, -0.454019], tstacc=0.1468\n",
      "Loss for #epoch : -0.25215968\n",
      "epoch=96 trnloss=[-0.238086, 0.0993304, 0.161214, -0.49863], trnacc=0.16121389, tstloss=[0.515486, 0.822061, 0.146483, -0.453058], tstacc=0.14648333\n",
      "Loss for #epoch : -0.2463141\n",
      "epoch=97 trnloss=[-0.195384, 0.137489, 0.159086, -0.491959], trnacc=0.15908611, tstloss=[0.634373, 0.933652, 0.143033, -0.442313], tstacc=0.14303334\n",
      "Loss for #epoch : -0.25570285\n",
      "epoch=98 trnloss=[-0.228891, 0.107479, 0.160722, -0.497093], trnacc=0.16072223, tstloss=[0.61736, 0.918571, 0.143917, -0.445128], tstacc=0.14391667\n",
      "Loss for #epoch : -0.25439632\n",
      "epoch=99 trnloss=[-0.236978, 0.10068, 0.161342, -0.498999], trnacc=0.16134167, tstloss=[0.561429, 0.865792, 0.145433, -0.449796], tstacc=0.14543334\n",
      "Loss for #epoch : -0.25417823\n",
      "epoch=100 trnloss=[-0.264534, 0.0753457, 0.162406, -0.502286], trnacc=0.16240555, tstloss=[0.474965, 0.782753, 0.147067, -0.454855], tstacc=0.14706667\n",
      "Loss for #epoch : -0.25757146\n",
      "epoch=101 trnloss=[-0.254482, 0.0847047, 0.162078, -0.501264], trnacc=0.16207778, tstloss=[0.486867, 0.795297, 0.147383, -0.455813], tstacc=0.14738333\n",
      "Loss for #epoch : -0.24913298\n",
      "epoch=102 trnloss=[-0.270478, 0.0700533, 0.162719, -0.50325], trnacc=0.16271944, tstloss=[0.468261, 0.77605, 0.147067, -0.454856], tstacc=0.14706667\n",
      "Loss for #epoch : -0.25886264\n",
      "epoch=103 trnloss=[-0.225206, 0.111039, 0.160683, -0.496928], trnacc=0.16068333, tstloss=[0.558123, 0.863841, 0.1461, -0.451818], tstacc=0.1461\n",
      "Loss for #epoch : -0.25645113\n",
      "epoch=104 trnloss=[-0.260416, 0.0791313, 0.162231, -0.501778], trnacc=0.16223055, tstloss=[0.497992, 0.806103, 0.147217, -0.455327], tstacc=0.14721666\n",
      "Loss for #epoch : -0.26015443\n",
      "epoch=105 trnloss=[-0.23031, 0.106787, 0.161103, -0.4982], trnacc=0.16110277, tstloss=[0.606599, 0.911393, 0.145667, -0.450461], tstacc=0.14566667\n",
      "Loss for #epoch : -0.25742802\n",
      "epoch=106 trnloss=[-0.252041, 0.0868412, 0.161919, -0.500802], trnacc=0.16191944, tstloss=[0.544166, 0.851559, 0.146867, -0.45426], tstacc=0.14686666\n",
      "Loss for #epoch : -0.25801745\n",
      "epoch=107 trnloss=[-0.247983, 0.0901655, 0.161603, -0.499752], trnacc=0.16160278, tstloss=[0.615879, 0.92059, 0.145633, -0.450345], tstacc=0.14563334\n",
      "Loss for #epoch : -0.25893477\n",
      "epoch=108 trnloss=[-0.25488, 0.0840532, 0.161967, -0.5009], trnacc=0.16196667, tstloss=[0.619761, 0.924077, 0.145433, -0.44975], tstacc=0.14543334\n",
      "Loss for #epoch : -0.25706482\n",
      "epoch=109 trnloss=[-0.266249, 0.0737906, 0.162489, -0.502528], trnacc=0.1624889, tstloss=[0.516257, 0.823923, 0.147017, -0.454683], tstacc=0.14701666\n",
      "Loss for #epoch : -0.25984573\n",
      "epoch=110 trnloss=[-0.266102, 0.0742489, 0.162653, -0.503004], trnacc=0.16265278, tstloss=[0.501633, 0.809259, 0.147017, -0.454642], tstacc=0.14701666\n",
      "Loss for #epoch : -0.2669652\n",
      "epoch=111 trnloss=[-0.236597, 0.101103, 0.161361, -0.499061], trnacc=0.16136111, tstloss=[0.652396, 0.954905, 0.14455, -0.447059], tstacc=0.14455\n",
      "Loss for #epoch : -0.25713018\n",
      "epoch=112 trnloss=[-0.254506, 0.0845676, 0.162014, -0.501088], trnacc=0.16201389, tstloss=[0.636825, 0.942753, 0.146167, -0.452095], tstacc=0.14616667\n",
      "Loss for #epoch : -0.26534155\n",
      "epoch=113 trnloss=[-0.22432, 0.112844, 0.161103, -0.498267], trnacc=0.16110277, tstloss=[0.654615, 0.959581, 0.145733, -0.450699], tstacc=0.14573333\n",
      "Loss for #epoch : -0.25994423\n",
      "epoch=114 trnloss=[-0.261531, 0.0785704, 0.162497, -0.502598], trnacc=0.16249722, tstloss=[0.569778, 0.877204, 0.146883, -0.454309], tstacc=0.14688334\n",
      "Loss for #epoch : -0.2615872\n",
      "epoch=115 trnloss=[-0.251004, 0.0883502, 0.162153, -0.501507], trnacc=0.16215278, tstloss=[0.663497, 0.968807, 0.145883, -0.451194], tstacc=0.14588334\n",
      "Loss for #epoch : -0.2638841\n",
      "epoch=116 trnloss=[-0.26488, 0.0752029, 0.162536, -0.502619], trnacc=0.16253611, tstloss=[0.597969, 0.902001, 0.145317, -0.449349], tstacc=0.14531666\n",
      "Loss for #epoch : -0.25877017\n",
      "epoch=117 trnloss=[-0.272212, 0.0684484, 0.162817, -0.503477], trnacc=0.16281667, tstloss=[0.632779, 0.936464, 0.14515, -0.448835], tstacc=0.14515\n",
      "Loss for #epoch : -0.2636765\n",
      "epoch=118 trnloss=[-0.241816, 0.0955347, 0.161261, -0.498612], trnacc=0.16126111, tstloss=[0.679031, 0.980499, 0.144117, -0.445585], tstacc=0.14411667\n",
      "Loss for #epoch : -0.26795486\n",
      "epoch=119 trnloss=[-0.14576, 0.185228, 0.158281, -0.489268], trnacc=0.15828055, tstloss=[0.90037, 1.19337, 0.140117, -0.433113], tstacc=0.14011666\n",
      "Loss for #epoch : -0.25958407\n",
      "epoch=120 trnloss=[-0.257437, 0.082339, 0.162342, -0.502118], trnacc=0.16234167, tstloss=[0.677235, 0.981782, 0.145517, -0.450064], tstacc=0.14551666\n",
      "Loss for #epoch : -0.2673245\n",
      "epoch=121 trnloss=[-0.267352, 0.073104, 0.162719, -0.503175], trnacc=0.16271944, tstloss=[0.608331, 0.915005, 0.146583, -0.453257], tstacc=0.14658333\n",
      "Loss for #epoch : -0.2627208\n",
      "epoch=122 trnloss=[-0.217085, 0.118812, 0.160594, -0.496492], trnacc=0.16059445, tstloss=[0.73655, 1.03521, 0.1428, -0.441456], tstacc=0.1428\n",
      "Loss for #epoch : -0.26533708\n",
      "epoch=123 trnloss=[-0.294627, 0.0483058, 0.163872, -0.506805], trnacc=0.16387223, tstloss=[0.527684, 0.838092, 0.148333, -0.458742], tstacc=0.14833333\n",
      "Loss for #epoch : -0.26934856\n",
      "epoch=124 trnloss=[-0.263843, 0.076342, 0.162578, -0.502763], trnacc=0.16257778, tstloss=[0.657401, 0.963197, 0.14615, -0.451946], tstacc=0.14615\n",
      "Loss for #epoch : -0.2658269\n",
      "epoch=125 trnloss=[-0.264461, 0.0757572, 0.162569, -0.502787], trnacc=0.16256945, tstloss=[0.588156, 0.893272, 0.1458, -0.450916], tstacc=0.1458\n",
      "Loss for #epoch : -0.2667183\n",
      "epoch=126 trnloss=[-0.248977, 0.0893632, 0.161758, -0.500099], trnacc=0.16175833, tstloss=[0.698728, 1.00002, 0.14405, -0.445343], tstacc=0.14405\n",
      "Loss for #epoch : -0.2683137\n",
      "epoch=127 trnloss=[-0.272361, 0.0688196, 0.163022, -0.504203], trnacc=0.16302222, tstloss=[0.646103, 0.95209, 0.1462, -0.452188], tstacc=0.1462\n",
      "Loss for #epoch : -0.2682382\n",
      "epoch=128 trnloss=[-0.287776, 0.0546196, 0.163664, -0.50606], trnacc=0.1636639, tstloss=[0.663052, 0.968288, 0.1459, -0.451136], tstacc=0.1459\n",
      "Loss for #epoch : -0.2733252\n",
      "epoch=129 trnloss=[-0.208954, 0.127648, 0.160839, -0.497441], trnacc=0.16083889, tstloss=[0.826364, 1.12749, 0.1439, -0.445031], tstacc=0.1439\n",
      "Loss for #epoch : -0.26470146\n",
      "epoch=130 trnloss=[-0.278688, 0.0630146, 0.163269, -0.504972], trnacc=0.16326945, tstloss=[0.649681, 0.955534, 0.14615, -0.452002], tstacc=0.14615\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "history = []\n",
    "loss(x, yref) = sum(ram(x, yref)[1:3])\n",
    "gradients = []\n",
    "loss_batch = []\n",
    "for epoch = 1:epochs\n",
    "    losses = []\n",
    "    for (x, y) in dtrn\n",
    "        lss = @diff loss(x, y)\n",
    "        push!(losses, value(lss))\n",
    "        for par in params(ram)\n",
    "            g = grad(lss, par)\n",
    "            update!(value(par), g, par.opt)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    push!(loss_batch, mean(losses))\n",
    "    trn_losses, trn_acc = validate(ram, dtrn);\n",
    "    tst_losses, tst_acc = validate(ram, dtst);\n",
    "    println(\"Loss for #epoch : \", mean(losses))\n",
    "        println(\n",
    "        \"epoch=$(1+length(history)) \",\n",
    "        \"trnloss=$(trn_losses), trnacc=$trn_acc, \",\n",
    "        \"tstloss=$(tst_losses), tstacc=$tst_acc\")\n",
    "    push!(history, ([trn_losses..., trn_acc, tst_losses..., tst_acc]));\n",
    "\n",
    "        \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Knet.@save \"samples.jld2\" LOCATION_RECORD SAMPLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Knet.save(FILENAME, \"RAM\", ram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Knet.@save \"HistoryBnorm_acc.jld2\" history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ram = nothing\n",
    "Knet.gc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.0.3",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
