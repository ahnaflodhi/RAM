{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ImageMagick\n",
    "using Knet\n",
    "using AutoGrad\n",
    "using JLD\n",
    "using Statistics\n",
    "using Random\n",
    "using Images\n",
    "include(Knet.dir(\"data\", \"mnist.jl\"))\n",
    "xtrain, ytrain, xtest, ytest = mnist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"History_Fleer.jld2\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Global Parameters\n",
    "BANDWIDTH = 8\n",
    "NUM_PATCHES = 1\n",
    "GLIMPSE_COUNT = 6 #Number of glimpses to be employed\n",
    "NUM_CLASSES = 10\n",
    "SCALE = 2\n",
    "HSIZE = 256\n",
    "GSIZE = 256;\n",
    "BATCHSIZE = 20\n",
    "MOdel_FILENAME = \"RAM_Fleer.jld2\"\n",
    "Data_FILENAME = \"History_Fleer.jld2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "denorm (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generic Functions\n",
    "# Normalize dataset and return mean and SD\n",
    "function Normalize(dataset)\n",
    "    mean_dataset = Float32(mean(xtrain))\n",
    "    sd_dataset = Float32(std(xtrain))\n",
    "    dataset_norm = (dataset .- mean_dataset) ./ sd_dataset\n",
    "    return dataset_norm\n",
    "end\n",
    "\n",
    "\n",
    "# Function for denormalizing the locations\n",
    "# From -1:1 to 1:28\n",
    "# Batch location dimensions = 2 x Batchsize\n",
    "function antinorm(dim, loc)\n",
    "    #Conver Normalized locations range[-1,1] into X, Y coordinates\n",
    "    anloc = []\n",
    "    meanx::Int16 = floor(dim/2);\n",
    "    meany::Int16 = floor(dim/2);\n",
    "    \n",
    "    locx = Array{Int16}(ceil.((loc[1,:] .* dim) ./ 2 .+ meanx))  \n",
    "    locy = Array{Int16}(ceil.((loc[2,:] .* dim) ./ 2 .+ meany))\n",
    "    for i = 1:BATCHSIZE\n",
    "        temp = reshape([locx[i], locy[i]], (2,1))\n",
    "        push!(anloc, temp)\n",
    "    end\n",
    "    return (cat(anloc..., dims = 2))\n",
    "    \n",
    "end\n",
    "\n",
    "function denorm(dim, loc)\n",
    "    return convert(Array{Int}, floor.(0.5 .* dim .* (loc .+ 1.0)))\n",
    "end\n",
    "\n",
    "\n",
    "# # AntiNorm test\n",
    "# test = KnetArray{Float32}(rand(2,BATCHSIZE))\n",
    "# println(size(test))\n",
    "# test = antinorm(28, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SDX = Float32(0.11)\n",
    "xtrain = Normalize(xtrain);\n",
    "xtest = Normalize(xtest);\n",
    "\n",
    "dtrn = minibatch(xtrain, ytrain, BATCHSIZE, xtype=KnetArray{Float32});\n",
    "dtst = minibatch(xtest, ytest, BATCHSIZE, xtype=KnetArray{Float32});\n",
    "\n",
    "# Samples for Testing\n",
    "xd, yd = first(dtrn);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BatchNorm"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################################################################################################\n",
    "struct FC\n",
    "    w\n",
    "    b\n",
    "    act\n",
    "end\n",
    "\n",
    "(f::FC)(x) = f.act.(f.w * mat(x) .+ f.b)\n",
    "FC(outsize::Int, insize::Int, act) = FC(Knet.param(outsize, insize, atype = KnetArray{Float32}), \n",
    "    Knet.param(outsize, atype = KnetArray{Float32}), act);\n",
    "\n",
    "\n",
    "# # Layer Test\n",
    "# F1 = FC(100, 784)\n",
    "# params(FC1)\n",
    "# testFC = FC1(xd)\n",
    "#################################################################################################\n",
    "\n",
    "struct Linear\n",
    "    w\n",
    "    b\n",
    "end\n",
    "\n",
    "(l::Linear)(x) = l.w * mat(x) .+ l.b\n",
    "\n",
    "function Linear(outsize::Int, insize::Int)\n",
    "    w = Knet.param(outsize, insize, atype=KnetArray{Float32})\n",
    "    b = Knet.param(outsize, atype = KnetArray{Float32})\n",
    "    return Linear(w,b)\n",
    "end\n",
    "\n",
    "# # Layer Test\n",
    "# L1 = Linear(100, 784)\n",
    "# params(L1)\n",
    "# testLinear = L1(xd)\n",
    "\n",
    "###################################################################################################\n",
    "\n",
    "mutable struct BatchNorm\n",
    "    w\n",
    "    m\n",
    "end\n",
    "\n",
    "(b::BatchNorm)(x) = batchnorm(x, b.m, b.w)\n",
    "\n",
    "function BatchNorm(c::Int; atype=KnetArray{Float32})\n",
    "    w = Knet.Param(atype(bnparams(c)))\n",
    "    m = bnmoments()\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Network\n",
    "Takes the output of Glimpse Network(gt -> gc) and previous hidden state(h(t-1)-> hp). Outputs current Hidden State (ht -> hc).\n",
    "Runs num_glimpses times and records the hidden state at the end as output. \n",
    "Unit Defintion = Rect (Linear(h(t-1)) + Linear(gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct CoreNet\n",
    "    FCgc   # Linear operator for g current  : gt\n",
    "    FChp   # Linear operator for h previous : ht-1\n",
    "end\n",
    "\n",
    "function CoreNet(gsize::Int, hsize::Int)\n",
    "    return CoreNet(Linear(hsize, hsize), Linear(hsize, hsize))\n",
    "end\n",
    "\n",
    "function (c::CoreNet)(gt,hp)\n",
    "    return relu.(c.FCgc(gt) + c.FChp(hp))\n",
    "end\n",
    "\n",
    "# CoreNet Test\n",
    "# CN = CoreNet(hsize, hsize)\n",
    "# sample = KnetArray{Float32}(rand(256,10))\n",
    "# sample = reshape(sample , (256, 10))\n",
    "# CN_test = CN(sample, sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glimpse Network\n",
    "## Glimpse Network Constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct GpsNet\n",
    "    FC1\n",
    "    FC2\n",
    "    FC3\n",
    "    FC4\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glimpse Network: Initializer & Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glimpse Network default constructor: Definition of vairous cogs of the Glimpse network\n",
    "function GpsNet(bandwidth::Int, scale::Int, num_patches::Int)\n",
    "    FC1 = FC(128, bandwidth*bandwidth*num_patches, relu)\n",
    "    FC2 = FC(128, 2, relu)\n",
    "    FC3 = Linear(256, 256)  # The outsize may be 256\n",
    "    FC4 = Linear(256, 256) # The outsize may be 256\n",
    "    return GpsNet(FC1, FC2, FC3, FC4)\n",
    "end\n",
    "\n",
    "###########################################################################################\n",
    "\n",
    "## Glimpse Network Deployment\n",
    "\n",
    "# Deploying glimpse Network\n",
    "# Input is the batch of images\n",
    "# Take glimpses & locations. Pass them to FC layers \n",
    "function (g::GpsNet)(x_batch, loc)\n",
    "    phi   = sensor(x_batch, loc)\n",
    "#     phi =  mat(phi)\n",
    "    phi_1 = g.FC1(phi)\n",
    "    loc_1 = g.FC2(loc)\n",
    "    cat_philoc =cat((loc_1, phi_1)..., dims =1)\n",
    "    cat1 = relu.(g.FC3(cat_philoc))\n",
    "    cat2 = g.FC4(cat1)\n",
    "    return cat2 # The output here is gt to be fed to CoreNet\n",
    "end\n",
    "\n",
    "# # GpsNet Test\n",
    "# test2 = GpsNet(bandwidth, scale, num_patches)\n",
    "# params(test2.FC2)\n",
    "# res_test2 = test2(xd, loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensor Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sensor (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uses imresize for equalizing the return dimensions\n",
    "function sensor(x_batch, loc)\n",
    "    l,w,c,b = size(x_batch)\n",
    "    coord = antinorm(l, loc)\n",
    "    glimpse_array = []\n",
    "    padsize::Int = 60\n",
    "    width::Int = floor(BANDWIDTH / 2)  \n",
    "    batch = Array(mat(x_batch))\n",
    "    batch = reshape(batch, (l,w,b))\n",
    "    \n",
    "    for i = 1:BATCHSIZE\n",
    "        img = batch[:,:,i]\n",
    "        pad_img = padarray(img, Fill(0, (padsize, padsize)))\n",
    "        temp = []\n",
    "        for j = 1: NUM_PATCHES\n",
    "            xlim1 = coord[1,i] - (width* (SCALE ^ (j-1)))\n",
    "            xlim2 = coord[1,i]-1 + (width * (SCALE ^(j-1)))\n",
    "            ylim1 = coord[2,i] - (width* (SCALE ^ (j-1)))\n",
    "            ylim2 = coord[2,i]-1 + (width * (SCALE ^(j-1)))\n",
    "\n",
    "            glimpse = pad_img[xlim1:xlim2, ylim1:ylim2]\n",
    "            glimpse = imresize(glimpse, (BANDWIDTH, BANDWIDTH))\n",
    "            push!(temp, glimpse)\n",
    "        end\n",
    "        temp = vcat(vcat(temp...)...)\n",
    "        push!(glimpse_array, vcat(temp))\n",
    "    end\n",
    "    return KnetArray{Float32}(mat(cat(glimpse_array..., dims=2)))\n",
    "end\n",
    "\n",
    "## Function test\n",
    "# coord = KnetArray{Float32}(zeros(2, BATCHSIZE))\n",
    "# res = sensor(xd, coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses the pooling for equalizing dimensions\n",
    "function sensor(x_batch, loc)\n",
    "    patch_size =  BANDWIDTH\n",
    "    phi = []\n",
    "    for i = 1:NUM_PATCHES\n",
    "        push!(phi, foveat(x_batch, loc, patch_size))\n",
    "        patch_size = SCALE * patch_size\n",
    "    end\n",
    "    \n",
    "    for j = 1:length(phi)\n",
    "        k = div(size(phi[j])[1], BANDWIDTH)\n",
    "        phi[j] = pool(phi[j]; window = k, mode=1)\n",
    "    end\n",
    "    \n",
    "    phi2d = map(mat, phi)\n",
    "    return mat(vcat(phi2d...))\n",
    "end\n",
    "\n",
    "function foveat(x_batch, loc, patch_size)\n",
    "    l,w,c,b = size(x_batch)\n",
    "    coord = antinorm(l, loc)\n",
    "    patches = [] \n",
    "    padsize::Int = 60\n",
    "    width::Int = floor(patch_size / 2)\n",
    "\n",
    "    xlim1 = coord[1,:] .- width .+1\n",
    "    xlim2 = coord[1,:] .+ width\n",
    "    ylim1 = coord[2,:] .- width .+1\n",
    "    ylim2 = coord[2,:] .+ width \n",
    "    batch = Array(x_batch)\n",
    "\n",
    "    for i=1:b\n",
    "        img = batch[:,:,1,i]\n",
    "        pad_img = padarray(img, Fill(0, (padsize, padsize)))\n",
    "        push!(patches, pad_img[xlim1[i]:xlim2[i], ylim1[i]:ylim2[i]])\n",
    "    end\n",
    "    \n",
    "    return KnetArray{Float32}(cat(patches...; dims=4))\n",
    "end          \n",
    "\n",
    "# ## Function test\n",
    "# coord = KnetArray{Float32}(zeros(2, BATCHSIZE))\n",
    "# res = sensor(xd, coord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct LocNet\n",
    "    sdx\n",
    "    Linl\n",
    "end\n",
    "\n",
    "# Function should take input size as 256 x BATCHSIZE and output size as 2 (size of loc coordinates) x BATCHSIZE\n",
    "function LocNet(outsize::Int, insize::Int, sdx::Float32)\n",
    "    Linloc = Linear(outsize, insize)\n",
    "    return LocNet(sdx, Linloc)\n",
    "end\n",
    "\n",
    "function (l::LocNet)(ht)\n",
    "    mean_loc = tanh.(l.Linl(value(ht)))\n",
    "    prtb = SDX .* randn!(similar(mean_loc))\n",
    "    loc_new = mean_loc + prtb\n",
    "#     loc_new = mean_loc + KnetArray{Float32}(randn((2, BATCHSIZE))) .* l.sdx\n",
    "    return mean_loc, tanh.(loc_new)\n",
    "end\n",
    "\n",
    "# function (l::LocNet)(ht)\n",
    "#     mean_loc = tanh.(l.Linl(ht))\n",
    "    \n",
    "#     loc_new = KnetArray{Float32}(zeros(size(mean_loc)))\n",
    "#     for i = 1:size(mean_loc)[1]\n",
    "#         for j = 1:size(mean_loc)[2]\n",
    "#             loc_new = rand(Normal(mean_loc[i,j], l.sdx))\n",
    "#         end\n",
    "#     end\n",
    "    \n",
    "#     return mean_loc, value(tanh.(loc_new))\n",
    "# end\n",
    "\n",
    "# # TLocation Network Test\n",
    "# Loc1 = LocNet(2, 256, SDX)\n",
    "# sample = KnetArray{Float32}(rand(256));\n",
    "# sample = reshape(sample, (256, 1));\n",
    "# res, lt = Loc1(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAM Model\n",
    "### Staggered RAM\n",
    "The Reference RAM deployment has been staggered.\n",
    "Deploy one RAM function once to generate the hidden state, next location and action.\n",
    "Deploy this one-time active RAM recursively equal to number of glimpses. Record all intermediate states\n",
    "but base decision on output of the final one of the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAM Initializer\n",
    "# ram = RAM(SDX, GLIMPSE_COUNT, BANDWIDTH, NUM_PATCHES, NUM_CLASSES, SCALE, GSIZE, HSIZE);\n",
    "mutable struct RAM\n",
    "    sdx\n",
    "    num_glimpses\n",
    "    gpsnet::GpsNet\n",
    "    corenet::CoreNet\n",
    "    locnet::LocNet\n",
    "    output_layer\n",
    "    locations\n",
    "    basenet\n",
    "    hsize\n",
    "end\n",
    "\n",
    "function RAM(sdx, glimpse_count, bandwidth, num_patches, num_classes, scale, gsize, hsize)\n",
    "    gpsnet = GpsNet(bandwidth, scale, num_patches)\n",
    "    corenet = CoreNet(gsize, hsize)\n",
    "    locnet = LocNet(2, hsize, sdx)\n",
    "    output_layer = Linear(num_classes, hsize) # Action Network\n",
    "    locations = []\n",
    "    basenet = Linear(1, hsize)\n",
    "    return RAM(sdx, glimpse_count, gpsnet, corenet, locnet, output_layer, locations, basenet, hsize)\n",
    "end\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "# Outer RAM instance. Calls inner instance glimpse_count times\n",
    "# Returns class_probs, policy result, hidden state and baseline results\n",
    "\n",
    "function (r::RAM)(x_batch)\n",
    "    # Init States\n",
    "    hc = KnetArray{Float32}(reshape(zeros(r.hsize, BATCHSIZE), (r.hsize, BATCHSIZE)))\n",
    "    loc = KnetArray{Float32}(zeros(2,BATCHSIZE))\n",
    "    log_ps, baseresults = [], []  \n",
    "    for i = 1:r.num_glimpses\n",
    "        hc, loc, policy, baseres = r(x_batch, loc, hc)\n",
    "        push!(log_ps, policy)       \n",
    "        push!(baseresults, baseres)      \n",
    "    end\n",
    "    log_ps, baseresults = vcat(log_ps...), vcat(baseresults...)    \n",
    "    scores = logsoftmax(r.output_layer(hc))  # Size of NUM_CLASSES x BATCHSIZE. mat() applied at layer definition\n",
    "    return scores, log_ps, hc, baseresults\n",
    "end\n",
    "\n",
    "###########################################################################################\n",
    "\n",
    "# Innermost RAM instance\n",
    "# Return for a single glimpse, hidden state, next_location, policy and baseline results.\n",
    "function (r::RAM)(x_batch, loc, hp)\n",
    "    gc = r.gpsnet(x_batch, loc)  # g-current: gt\n",
    "    hc = r.corenet(gc,hp)       # h-current ; ht-- Uses g_current and h_previous\n",
    "    mean_loc, lnext = r.locnet(hc) # mean and l-next:lt to be used in the next time step\n",
    "    sdx2 = (r.sdx.^2)\n",
    "#     policy = -(abs.(lnext - mean_loc).^2) ./ 2*(sdx2) .- log(r.sdx) .- log(sqrt(2 * 3.1415))\n",
    "#     policy = (value(lnext) - mean_loc) ./ sdx2 # #Glimpses x \n",
    "    policy = (value(lnext) - mean_loc) ./ sdx2\n",
    "    policy = sum(policy, dims =1)\n",
    "    base_res = sigm.(r.basenet(hc))\n",
    "    return hc, lnext, policy, base_res\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Caluclator\n",
    "function (ram::RAM)(x_batch, y)\n",
    "    scores, log_ps, hc, baseresults = ram(x_batch) \n",
    "    ypred = vec(map(i->i[1], argmax(Array(value(scores)), dims=1)))\n",
    "\n",
    "    r = (ypred .== y)\n",
    "    r = reshape(r, 1, :) \n",
    "    \n",
    "    # Modified Implementation\n",
    "    R = zeros(Float32, size(baseresults)...); R[end,:] = r\n",
    "    R = KnetArray{Float32}(R)    \n",
    "    \n",
    "    # Batch Losses\n",
    "    R_err = (R - baseresults)\n",
    "    loss_action = nll(scores, y)\n",
    "    loss_baseline = sum(abs2, value(baseresults) .- R) / length(baseresults)\n",
    "    \n",
    "    # Reinforce Loss\n",
    "#     mean_logps = mean(log_ps)\n",
    "#     loss_reinforce = sum(mean_logps .* R_err)\n",
    "    \n",
    "    # Reinforce Loss 2\n",
    "    loss_reinforce = mean(sum(-log_ps .* R_err, dims =1))\n",
    "    return loss_action, loss_baseline, loss_reinforce, sum(R), length(R)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "validate (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function validate(ram::RAM, data)\n",
    "    losses = zeros(3)\n",
    "    ncorrect = ninstances = 0\n",
    "    for (x,y) in data\n",
    "        ret = ram(x,y)\n",
    "        for i = 1:3; losses[i] += ret[i]; end\n",
    "        ncorrect += ret[4]\n",
    "        ninstances += ret[5]\n",
    "    end\n",
    "    losses = losses / length(data)\n",
    "    losses = [sum(losses), losses...]\n",
    "    return losses, ncorrect / ninstances\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAM Initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "function initopt!(model, optimizer=\"Adam(lr=0.001, gclip = 5)\")\n",
    "    for par in params(model)\n",
    "        par.opt = eval(Meta.parse(optimizer))\n",
    "    end\n",
    "end\n",
    "\n",
    "ram = RAM(SDX, GLIMPSE_COUNT, BANDWIDTH, NUM_PATCHES, NUM_CLASSES, SCALE, GSIZE, HSIZE);\n",
    "record = params(ram);\n",
    "initopt!(ram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ram = Knet.load(FILENAME, \"RAM\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for #epoch : -22.350313\n",
      "epoch=1 trnloss=[-28.0982, 0.93616, 0.12143, -29.1558], trnacc=0.12143055, tstloss=[-11.335, 1.97082, 0.0555162, -13.3614], tstacc=0.055516668\n",
      "Loss for #epoch : -30.787079\n",
      "epoch=2 trnloss=[-30.7534, 0.731827, 0.131592, -31.6168], trnacc=0.13159166, tstloss=[-14.1088, 1.83736, 0.0664999, -16.0127], tstacc=0.0665\n",
      "Loss for #epoch : -32.037098\n",
      "epoch=3 trnloss=[-31.8658, 0.632044, 0.135744, -32.6336], trnacc=0.13574444, tstloss=[-14.9966, 1.77776, 0.0700833, -16.8445], tstacc=0.070083335\n",
      "Loss for #epoch : -32.735504\n",
      "epoch=4 trnloss=[-32.4518, 0.600307, 0.138169, -33.1903], trnacc=0.13816944, tstloss=[-10.8909, 2.06365, 0.0540333, -13.0086], tstacc=0.05403333\n",
      "Loss for #epoch : -33.22399\n",
      "epoch=5 trnloss=[-32.7726, 0.574451, 0.139433, -33.4865], trnacc=0.13943334, tstloss=[-9.70992, 2.69893, 0.05195, -12.4608], tstacc=0.05195\n",
      "Loss for #epoch : -33.61443\n",
      "epoch=6 trnloss=[-33.364, 0.519892, 0.141539, -34.0254], trnacc=0.14153889, tstloss=[-12.6103, 2.30332, 0.06245, -14.976], tstacc=0.06245\n",
      "Loss for #epoch : -33.878925\n",
      "epoch=7 trnloss=[-33.7426, 0.492297, 0.142881, -34.3777], trnacc=0.14288056, tstloss=[-7.32983, 2.56816, 0.0413833, -9.93938], tstacc=0.041383334\n",
      "Loss for #epoch : -34.056103\n",
      "epoch=8 trnloss=[-33.8884, 0.465901, 0.143631, -34.4979], trnacc=0.14363055, tstloss=[-10.5544, 2.74602, 0.0554167, -13.3559], tstacc=0.055416666\n",
      "Loss for #epoch : -34.291874\n",
      "epoch=9 trnloss=[-33.7164, 0.491405, 0.142928, -34.3507], trnacc=0.14292778, tstloss=[-6.61737, 3.36577, 0.0417667, -10.0249], tstacc=0.041766666\n",
      "Loss for #epoch : -34.442417\n",
      "epoch=10 trnloss=[-34.2252, 0.432106, 0.14495, -34.8023], trnacc=0.14495, tstloss=[-5.94373, 4.08969, 0.0419333, -10.0753], tstacc=0.04193333\n",
      "Loss for #epoch : -34.6103\n",
      "epoch=11 trnloss=[-34.2111, 0.448444, 0.144931, -34.8045], trnacc=0.14493056, tstloss=[-9.66524, 4.80528, 0.0606167, -14.5311], tstacc=0.06061667\n",
      "Loss for #epoch : -34.810577\n",
      "epoch=12 trnloss=[-33.9509, 0.484859, 0.144028, -34.5798], trnacc=0.14402778, tstloss=[3.70592, 8.84446, 0.02155, -5.16009], tstacc=0.02155\n",
      "Loss for #epoch : -34.86853\n",
      "epoch=13 trnloss=[-34.086, 0.473033, 0.144525, -34.7035], trnacc=0.144525, tstloss=[1.80654, 8.60617, 0.02835, -6.82798], tstacc=0.02835\n",
      "Loss for #epoch : -35.01481\n",
      "epoch=14 trnloss=[-34.3745, 0.431475, 0.145608, -34.9516], trnacc=0.14560834, tstloss=[-1.45008, 6.79502, 0.0344167, -8.27952], tstacc=0.034416668\n",
      "Loss for #epoch : -35.121582\n",
      "epoch=15 trnloss=[-34.5248, 0.425906, 0.146019, -35.0967], trnacc=0.14601944, tstloss=[7.44345, 12.7191, 0.022, -5.29764], tstacc=0.022\n",
      "Loss for #epoch : -35.17361\n",
      "epoch=16 trnloss=[-34.2972, 0.456764, 0.145392, -34.8994], trnacc=0.14539167, tstloss=[3.26578, 9.6075, 0.0264167, -6.36814], tstacc=0.026416667\n",
      "Loss for #epoch : -35.31843\n",
      "epoch=17 trnloss=[-35.0587, 0.388448, 0.147989, -35.5951], trnacc=0.14798889, tstloss=[10.1055, 14.0815, 0.01655, -3.99259], tstacc=0.01655\n",
      "Loss for #epoch : -35.388542\n",
      "epoch=18 trnloss=[-34.823, 0.405081, 0.14725, -35.3753], trnacc=0.14725, tstloss=[11.0567, 15.0563, 0.0166667, -4.01628], tstacc=0.016666668\n",
      "Loss for #epoch : -35.46826\n",
      "epoch=19 trnloss=[-35.1525, 0.37804, 0.148408, -35.679], trnacc=0.14840834, tstloss=[11.2897, 17.9473, 0.0277667, -6.68532], tstacc=0.027766667\n",
      "Loss for #epoch : -35.60275\n",
      "epoch=20 trnloss=[-35.3347, 0.369974, 0.149125, -35.8538], trnacc=0.149125, tstloss=[1.44492, 7.0563, 0.0234833, -5.63486], tstacc=0.023483334\n",
      "Loss for #epoch : -35.6064\n",
      "epoch=21 trnloss=[-34.7614, 0.402652, 0.147217, -35.3113], trnacc=0.14721666, tstloss=[12.5347, 16.4558, 0.01625, -3.93736], tstacc=0.01625\n",
      "Loss for #epoch : -35.718876\n",
      "epoch=22 trnloss=[-35.2052, 0.382441, 0.148778, -35.7364], trnacc=0.14877778, tstloss=[4.56099, 12.6234, 0.0336, -8.09597], tstacc=0.0336\n",
      "Loss for #epoch : -35.705772\n",
      "epoch=23 trnloss=[-35.3029, 0.377299, 0.149058, -35.8293], trnacc=0.14905833, tstloss=[-0.481947, 9.55871, 0.0421333, -10.0828], tstacc=0.042133335\n",
      "Loss for #epoch : -35.805668\n",
      "epoch=24 trnloss=[-35.2629, 0.364202, 0.148822, -35.7759], trnacc=0.14882222, tstloss=[8.65138, 13.314, 0.0195667, -4.68222], tstacc=0.019566666\n",
      "Loss for #epoch : -35.881126\n",
      "epoch=25 trnloss=[-35.1404, 0.388034, 0.148372, -35.6768], trnacc=0.14837222, tstloss=[-2.50416, 6.38003, 0.03705, -8.92124], tstacc=0.03705\n",
      "Loss for #epoch : -35.904514\n",
      "epoch=26 trnloss=[-35.6951, 0.336731, 0.150497, -36.1824], trnacc=0.15049723, tstloss=[-1.53598, 5.17929, 0.0281333, -6.74341], tstacc=0.028133333\n",
      "Loss for #epoch : -35.994587\n",
      "epoch=27 trnloss=[-35.6495, 0.345807, 0.150386, -36.1457], trnacc=0.15038611, tstloss=[0.62306, 9.55347, 0.03735, -8.96776], tstacc=0.03735\n",
      "Loss for #epoch : -36.023228\n",
      "epoch=28 trnloss=[-35.3885, 0.410241, 0.149656, -35.9484], trnacc=0.14965555, tstloss=[3.81397, 11.3277, 0.03155, -7.5453], tstacc=0.03155\n",
      "Loss for #epoch : -36.010296\n",
      "epoch=29 trnloss=[-35.0227, 0.426863, 0.148192, -35.5978], trnacc=0.14819166, tstloss=[-4.91608, 6.41876, 0.0475667, -11.3824], tstacc=0.047566667\n",
      "Loss for #epoch : -36.13114\n",
      "epoch=30 trnloss=[-35.5564, 0.390372, 0.150264, -36.0971], trnacc=0.15026389, tstloss=[1.12129, 8.23994, 0.0297, -7.14836], tstacc=0.0297\n",
      "Loss for #epoch : -36.158764\n",
      "epoch=31 trnloss=[-35.561, 0.422691, 0.150339, -36.134], trnacc=0.15033889, tstloss=[6.95773, 12.1827, 0.0219333, -5.24688], tstacc=0.021933334\n",
      "Loss for #epoch : -36.20599\n",
      "epoch=32 trnloss=[-35.3858, 0.384615, 0.149625, -35.9201], trnacc=0.149625, tstloss=[9.26266, 16.0182, 0.0284333, -6.78398], tstacc=0.028433334\n",
      "Loss for #epoch : -36.23293\n",
      "epoch=33 trnloss=[-35.801, 0.352214, 0.151158, -36.3044], trnacc=0.15115833, tstloss=[9.46853, 15.5671, 0.0255667, -6.12412], tstacc=0.025566667\n",
      "Loss for #epoch : -36.254593\n",
      "epoch=34 trnloss=[-35.6441, 0.349079, 0.150506, -36.1436], trnacc=0.15050556, tstloss=[1.02356, 7.47474, 0.0270667, -6.47824], tstacc=0.027066667\n",
      "Loss for #epoch : -36.303223\n",
      "epoch=35 trnloss=[-35.8075, 0.342459, 0.151125, -36.3011], trnacc=0.151125, tstloss=[4.41817, 10.9641, 0.0273833, -6.57333], tstacc=0.027383333\n",
      "Loss for #epoch : -36.36051\n",
      "epoch=36 trnloss=[-35.6566, 0.37864, 0.150639, -36.1859], trnacc=0.1506389, tstloss=[3.06616, 8.22244, 0.02145, -5.17773], tstacc=0.02145\n",
      "Loss for #epoch : -36.379974\n",
      "epoch=37 trnloss=[-35.8165, 0.347158, 0.151314, -36.315], trnacc=0.15131389, tstloss=[8.971, 12.3111, 0.01395, -3.35406], tstacc=0.01395\n",
      "Loss for #epoch : -36.411858\n",
      "epoch=38 trnloss=[-35.6585, 0.337602, 0.150597, -36.1467], trnacc=0.15059723, tstloss=[4.36497, 10.4453, 0.02545, -6.10575], tstacc=0.02545\n",
      "Loss for #epoch : -36.497288\n",
      "epoch=39 trnloss=[-35.8144, 0.363701, 0.151158, -36.3292], trnacc=0.15115833, tstloss=[3.65634, 8.7603, 0.0214, -5.12536], tstacc=0.0214\n",
      "Loss for #epoch : -36.46439\n",
      "epoch=40 trnloss=[-35.7898, 0.327479, 0.150836, -36.2681], trnacc=0.15083611, tstloss=[8.09041, 13.2175, 0.0213167, -5.1484], tstacc=0.021316666\n",
      "Loss for #epoch : -36.595245\n",
      "epoch=41 trnloss=[-36.0433, 0.313397, 0.152086, -36.5088], trnacc=0.15208611, tstloss=[5.61886, 11.3501, 0.0239167, -5.75511], tstacc=0.023916667\n",
      "Loss for #epoch : -36.605923\n",
      "epoch=42 trnloss=[-35.8988, 0.3495, 0.151586, -36.3999], trnacc=0.15158612, tstloss=[3.25817, 9.91554, 0.0278333, -6.6852], tstacc=0.027833333\n",
      "Loss for #epoch : -36.613293\n",
      "epoch=43 trnloss=[-36.1272, 0.303544, 0.152411, -36.5832], trnacc=0.15241112, tstloss=[1.09973, 7.74023, 0.0278, -6.6683], tstacc=0.0278\n",
      "Loss for #epoch : -36.733807\n",
      "epoch=44 trnloss=[-35.8612, 0.395885, 0.151581, -36.4086], trnacc=0.15158056, tstloss=[0.14064, 6.84943, 0.0280667, -6.73686], tstacc=0.028066667\n",
      "Loss for #epoch : -36.627224\n",
      "epoch=45 trnloss=[-36.1709, 0.306244, 0.152472, -36.6296], trnacc=0.15247223, tstloss=[5.93819, 10.0266, 0.0169667, -4.10534], tstacc=0.016966667\n",
      "Loss for #epoch : -36.85339\n",
      "epoch=46 trnloss=[-36.0594, 0.333044, 0.152022, -36.5444], trnacc=0.15202223, tstloss=[1.56217, 7.43013, 0.0245, -5.89245], tstacc=0.0245\n",
      "Loss for #epoch : -36.871967\n",
      "epoch=47 trnloss=[-35.6999, 0.369892, 0.150836, -36.2206], trnacc=0.15083611, tstloss=[-0.541342, 5.27914, 0.0242667, -5.84475], tstacc=0.024266666\n",
      "Loss for #epoch : -36.80653\n",
      "epoch=48 trnloss=[-36.2881, 0.305928, 0.152947, -36.747], trnacc=0.15294722, tstloss=[7.33432, 11.2629, 0.0166, -3.94516], tstacc=0.0166\n",
      "Loss for #epoch : -36.91056\n",
      "epoch=49 trnloss=[-36.3394, 0.295075, 0.153144, -36.7876], trnacc=0.15314445, tstloss=[5.20527, 9.68077, 0.0188667, -4.49437], tstacc=0.018866668\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "history = []\n",
    "loss(x, yref) = sum(ram(x, yref)[1:3])\n",
    "gradients = []\n",
    "best_acc = 0.0\n",
    "for epoch = 1:epochs\n",
    "    losses = []\n",
    "    for (x, y) in dtrn\n",
    "        lss = @diff loss(x, y)\n",
    "        push!(losses, value(lss))\n",
    "        for par in params(ram)\n",
    "            g = grad(lss, par)          \n",
    "#             push!(gradients, g)\n",
    "            update!(value(par), g, par.opt)\n",
    "        end\n",
    "    end\n",
    "    println(\"Loss for #epoch : \", mean(losses))\n",
    "    #progress!(sgd(ram, dtrn; lr=0.001, gclip=0))\n",
    "    trn_losses, trn_acc = validate(ram, dtrn);\n",
    "    tst_losses, tst_acc = validate(ram, dtst);\n",
    "    println(\n",
    "        \"epoch=$(1+length(history)) \",\n",
    "        \"trnloss=$(trn_losses), trnacc=$trn_acc, \",\n",
    "        \"tstloss=$(tst_losses), tstacc=$tst_acc\")\n",
    "    push!(history, ([trn_losses..., trn_acc, tst_losses..., tst_acc]));\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Knet.save(\"RAM_Fleer.jld2\", \"RAM\", ram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Knet.@save \".jld2\" history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ram = nothing\n",
    "Knet.gc()"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.0.3",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
